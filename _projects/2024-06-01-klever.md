---
title: "Klever ‚Äî A Community-Driven AI Usability Testing Figma Plugin"
subtitle: "Figma + AI project (2024)"
date: 2024-06-01 00:00:00
description: "Klever showcases my journey from concept to creation, developing an AI-powered Figma plugin for usability testing. Community engagement was pivotal in its evolution, with its potential recognized by corporate entities üéâ"
featured_image: "2024-klever/hero.png"
gallery_images: "2024-klever/hero.png"
team: JooHyung(Lead), DongYeon Lee (Product Designer)
role: Product Lead
visible: true
---

The Mirror of Your Mind (Î™ÖÏã¨Î≥¥Í∞ê/ÊòéÂøÉÂØ∂Èëë) is a book that encapsulates the wisdom of Eastern philosophy. It teaches that to live a truly human life, one must broaden their learning, maintain a sincere purpose, never lose curiosity, keep questioning, and solve problems step by step. These teachings resonate deeply with us, product designers, living in the 21st-century digital and AI age. After all, it‚Äôs crucial to maintain a positive mindset even in **moments of change**.

{% include post-components/quote.html
   full_width = true
   background_color = "#FDF0E0"
   font_color = "#555"
   images = "../images/projects/2024-klever/6_logo_1_2.png"
   text = "It‚Äôs another morning, and as sunlight spills through my window, I fire up my laptop. Slack is buzzing with AI news. Curiosity piqued, I click on a few. Watching these AI design tool demos, a chill runs down my spine ‚Äî they‚Äôre doing what I do. It makes me wonder ‚Äî <br/><br/><span class='typeIt' style='font-size: 24px; background-color: #333; color: #fff; padding: 2px 4px;'>‚ÄúIs AI really going to replace us designers?‚Äù</span> <br/><br/>I‚Äôm not entirely sure, but all I know is<strong>I want to keep doing my best, today and tomorrow.</strong>"

%}
<br/>
<br/>
In a cozy nook of a Starbucks in western Singapore, two Grab designers, [DY LEE](https://www.linkedin.com/in/dy-lee/) and [Jude Park](https://www.linkedin.com/in/dusskapark/), kicked off what was just a humble AI side project. It wasn‚Äôt about achieving something monumental; it was about confronting the genuine anxieties of diving into AI, maintaining a hopeful perspective, and exploring new avenues for growth in an AI-driven landscape.


{::options parse_block_html="true" /}
<div class="timeline-section full-width align-left">

## üí° The Ideation Journey

<div class="outer">
<div class="timeline-item">
<span class="timeline-date">Jan 2024: The Question</span>
### Can AI Enhance Design?

As we move through 2024, AI is everywhere, and it‚Äôs changing how we work in design. Every day, new AI tools and models are emerging, transforming the way we approach our work. We initially thought AI would take over the boring tasks, freeing us to focus on creativity. However, AI is now involved in everything from generating creative content to designing user interfaces, reshaping our processes obviously.

In light of these changes, we found ourselves asking:

{::options parse_block_html="false" /}
{% include post-components/quote.html
   full_width = false
   background_color = "#000"
   font_color = "#fff"
   images = "../images/projects/2024-klever/6_design_process_4.png"
   text = "<span class='typeIt' style='font-size: 24px; background-color: #333; color: #fff; padding: 2px 4px;'>How can we embrace the power of AI in our design process?</span> <br/><br/>Beyond just automating tasks, what role can AI play in supporting us as designer assistants?"
%}
{::options parse_block_html="true" /}
<br/>
<br/>

We didn't find the perfect answer right away, but our exploration led us to a crucial realization: being a product designer is about more than just creating visual designs. It's about identifying challenges, devising solutions, and deeply understanding user needs and behaviors.

Instead of letting AI dictate our design decisions, we began to explore how we could harness its power to strengthen our insights and gain a deeper understanding of user behavior. This shift in perspective opened up new possibilities, leading us to discover Tencent's research on AppAgent - a finding that felt perfectly timed for our needs.
</div>

<div class="timeline-item">
<span class="timeline-date">Feb 2024: The Discovery</span>
### Meeting AppAgent and the Eureka Moment
{::options parse_block_html="false" /}
<div style="border: 1px solid #ccc; border-radius: 8px; padding: 16px; margin: 20px 0; display: flex; align-items: center; background-color: #f9f9f9;">
    <img src="https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg" alt="GitHub Icon" style="width: 40px; height: 40px; margin-right: 16px;">
    <div>
        <p style="margin: 0; font-size: 16px; font-weight: bold;">AppAgent: Multimodal Agents as Smartphone Users</p>
        <a href="https://appagent-official.github.io" style="color: #0366d6; text-decoration: none;">appagent-official.github.io</a>
    </div>
</div>
{::options parse_block_html="true" /}

Tencent‚Äôs research team introduced AppAgent, a multi-modal agent capable of interacting with apps naturally. Using LLM and Android Studio, AppAgent demonstrates how AI can explore apps and use gestures like a human.

As designers, we found ourselves captivated by AppAgent‚Äôs remarkable ability to simulate user behavior. It was as if we were watching real users interact with an app, tapping and swiping their way through the interface. This AI-driven mimicry of human interaction not only sparked our curiosity but also opened our minds to new creative possibilities.

{::options parse_block_html="false" /}  
{% include post-components/quote.html
   full_width = false
   background_color = "#FFF"
   font_color = "#555"
   images = "https://appagent-official.github.io/static/teaser.png"
   text = "This led us to ponder an exciting question:
<br><br><span class='typeIt' style='font-size: 24px; background-color: #333; color: #fff; padding: 2px 4px;'>What new horizons could we explore by integrating AppAgent with our Figma designs?</span>"
%}
{::options parse_block_html="true" /}
<br/>
<br/>

The idea of leveraging AI-generated user flows and behaviors in our UX research was compelling. It promised a more efficient way to address user challenges and conduct iterative testing, allowing us to refine our designs with greater precision.

To bring this vision to life, we knew we needed insights from our peers. We reached out to fellow designers, eager to gather their perspectives and explore how AppAgent could revolutionize our design process.
</div>

<div class="timeline-item">
<span class="timeline-date">Feb 2024: The Validation</span>
### Designer Interviews

We wanted to understand how AI & AppAgent could naturally fit into our design process, so we spoke with 10 designers across 7 teams over two weeks. Our goal was to uncover the challenges they face and see where AppAgent might offer solutions.

![Our interview journey: we spoke with 10 designers across 7 teams over two weeks](../images/projects/2024-klever/6_design_process_5.png)

From these interviews, we uncovered three key insights:

1. **Research is Time-Consuming:** User research often requires significant time and resources.
2. **Lack of Alternatives**: Designers struggle to find varied and effective alternatives when testing new concepts.
3. **Potential for Instant Testing:** AppAgent could greatly improve the speed and quality of usability testing.

AI is not a magic wand that solves every problem. The key is understanding how AI can add real value to the design process. Through conversations with designers, we discovered the potential for AppAgent to positively transform the way we design.
</div>

<div class="timeline-item">
<span class="timeline-date">Mar 2024: The Vision</span>
### Instant Usability Testing

We decided to set out to create a seamless connection between design and testing with AppAgent. Typically, testing designs requires organizing user sessions and justifying the time and resources involved. Convincing engineers to test different versions can also be a hurdle. However, what designers really want to know is ‚Äî **‚ÄúIs this design delivering a great user experience?‚Äù**

By integrating AppAgent with Figma‚Äôs prototype screens, we envisioned a tool that could answer those kinds of questions more easily.

{::options parse_block_html="false" /}  
{% include post-components/quote.html
   full_width = false
   background_color = "#000"
   font_color = "#fff"
   images = "../images/projects/2024-klever/6_design_process_6.png"
   text = "**_Without the need for recruiting users_** or **_spending extensive time on interviews_**, designers could gain insights by observing AI-generated user behaviors within seconds.<br><br><span style='font-size: 24px; background-color: #333; color: #fff; padding: 2px 4px;'>== The instant usability testing</span>, a game changer."
%}
<br/>
<br/>

This isn‚Äôt just about passing or failing a QA test. It‚Äôs about using AI to mimic real user interactions, allowing it to navigate through designs and suggest optimal paths with minimal clicks, or to point out areas that need refinement. AppAgent performs heuristic evaluations on prototypes, delivering clear and actionable insights without requiring lengthy business justifications.

Our task now is to bring this concept to life.
</div>
</div>
</div>
{::options parse_block_html="false" /}

## üõ†Ô∏è The Development Journey

Integrating AppAgent with Figma's prototype feature presented several significant technical hurdles. As this started as a small side project by designers, expanding from an Android app to Figma required us to overcome multiple complex challenges.

{::options parse_block_html="true" /}
<div class="timeline-section full-width align-right">
<div class="outer">
<div class="timeline-item">
<span class="timeline-date">Mar 2024: Understanding</span>
### How AppAgent Works?

Our initial challenge was to grasp the fundamental workings of AppAgent. The original AppAgent operates in a cycle of Observation, Thought, and Action, iterating multiple times.

{::options parse_block_html="false" /}
{% include post-components/gallery.html
   columns = 1
   full_width = false
   images = "../images/projects/2024-klever/5_demo_vid_0.png"
   caption="The original AppAgent operates: Observation ‚Üí Thought ‚Üí Action"
%}
{::options parse_block_html="true" /}


AppAgent provides a screenshot with UI annotations and a task to the GPT-4-vision model, which observes, thinks, and suggests the next action. This process repeats, simulating user interaction.

To adapt this for Figma prototypes, we faced two critical challenges:
- **Creating screenshot images and annotations for UI elements in Figma prototypes**
- **Implementing the AI model's suggested actions within Figma prototypes**

Understanding these core operations was essential to integrating AppAgent with Figma, setting the stage for our development journey.
</div>

<div class="timeline-item">
<span class="timeline-date">Mar 2024: Experimenting #1</span>
### Creating screenshot images and annotations for UI elements in Figma prototypes

Creating screenshot images and annotations in Figma prototypes wasn't too difficult. We used Figma's REST API to fetch node data from Figma design, convert it to images, and generate screenshot images and annotations.

{::options parse_block_html="false" /}
{% include post-components/gallery.html
   columns = 1
   full_width = false
   images = "../images/projects/2024-klever/5_demo_case%20study_1_0.png"
   caption="A sample screenshot vs A screenshot with UI object annotations"
%}
{::options parse_block_html="true" /}

However, implementing the resulting actions in Figma prototypes proved challenging. This was because Figma prototypes render their designs on `<Canvas>` like PNG images, making it difficult to recognize and interact with just UI components.
</div>

<div class="timeline-item">
<span class="timeline-date">Mar 2024: Experimenting #2</span>
### Implementing the AI model‚Äôs suggested actions within Figma prototypes

The solution was [Selenium](https://www.selenium.dev/). As a tool for controlling web browsers and interacting with web page UI elements, we created a `SeleniumController` to control Figma prototypes at the web browser level.

```python
# AppAgent/scripts/figma_controller.py

class SeleniumController:
   def __init__(self, url, password):
       self.url = url
       self.password = password
       self.driver = None

   def execute_selenium(self):
       options = Options()
       options.add_argument("user-data-dir=./User_Data")
       options.add_argument("disable-blink-features=AutomationControlled")
       options.add_argument("--start-maximized")
       options.add_experimental_option("detach", True)

       service = Service(ChromeDriverManager().install())
       self.driver = webdriver.Chrome(service=service, options=options)

       self.driver.get(self.url)
       # ...
```

By structuring the SeleniumController similarly to the AndroidController, we enabled control over Figma prototypes. We integrated this with the existing logic to run it as a new option, distinct from the Android-based approach.
</div>

<div class="timeline-item">
<span class="timeline-date">Mar 2024: Working Demo</span>
### The first prototype of &nbsp;<span style="background-color: #333; color: #fff; padding: 2px 4px;">AppAgent for Figma</span> 

After several trials and errors, we successfully developed a new option where AppAgent controls Figma prototypes, recognizes UI elements, and interacts with them, as demonstrated in the YouTube video below.
<br><br>

<iframe width="560" height="315" src="https://www.youtube.com/embed/1HuCiL9el3I?si=JX9BM_xb_wfAkYUq" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div>

<div class="timeline-item">
<span class="timeline-date">Jun 2024: Showcasing</span>
### Showcased at community events and got feedback to move forward

We gathered these interesting findings and introduced them to many AI-interested designers at the [Friends of Figma Seoul & Educators event](https://friends.figma.com/events/details/figma-seoul-presents-product-design-education-in-ai-era-figma-meetup-in-korea/cohost-seou) held in June 2024. The response from the attendees was truly enthusiastic. They showed great interest and excitement about how AI could innovate the design process. Many of them were eager to try it out.



<br><br>
However, we also discovered an important challenge. The biggest feedback was about Python. Many designers were fascinated by the AI instant testing‚Äôs features but felt a significant barrier to entry in having to set up a Python dev environment to use it.
</div>

<div class="timeline-item">
<span class="timeline-date">Jun 2024: Evolutioning</span>
### From experiment to product, <span style="background-color: #333; color: #fff; padding: 2px 4px;">Klever</span> was born

Based on this feedback, we made a bold decision. We decided to develop a Figma plugin version. This wasn‚Äôt just about changing platforms, but about moving beyond our ‚Äòexperiment‚Äô of applying Tencent‚Äôs paper to designers‚Äô environments and creating a ‚Äòproduct‚Äô that would make AppAgent easily accessible for all product designers.

So we decided to give this new tool a new identity. That‚Äôs how ‚ÄòKlever‚Äô was born. 

{::options parse_block_html="false" /}
{% include post-components/quote.html
   full_width = false
   background_color = "#000"
   font_color = "#fff"
   images = "../images/projects/2024-klever/6_logo_1_5.png"
   text = "Klever is a revolutionary Figma plugin that harnesses the power of AppAgent's AI technology while offering a seamless, designer-friendly experience. <br><br>It empowers designers to conduct instant usability testing and gain valuable insights, all within their familiar Figma environment."
%}
{::options parse_block_html="true" /}

---

In developing the Klever plugin, we faced many technical challenges. The biggest challenge was how to implement AppAgent‚Äôs core functionality in the limited environment of a Figma plugin. Some notable challenges include:

- **Transition to TypeScript:**
- **Development of the UIs:**
- **Utilizing Figma Plugin API:**

It wasn‚Äôt easy for us, designers, to code it to overcome these challenges as well, but we developed the Klever plugin while tackling these difficulties with Copilot. And through this, we made it possible to use AppAgent‚Äôs core functionality in Figma plugins as well.

<iframe width="560" height="315" src="https://www.youtube.com/embed/pZSXt8Cc4Ak?si=Pyc6zeVGFWv2Gvne" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
<br><br>

Please check out the Klever plugin through the link below and give us feedback!

{::options parse_block_html="true" /}
<div style="text-align: center; margin-top: 20px;">
  <a href="https://www.figma.com/community/plugin/1383457529531594701/klever-instant-usability-testing" target="_blank" style="display: inline-flex; max-width: 500px; max-height: 50px; align-items: center; padding: 10px 20px; background-color: #333333; color: white; text-decoration: none; border-radius: 5px; font-weight: bold;">
    <img src="https://upload.wikimedia.org/wikipedia/commons/3/33/Figma-logo.svg" alt="Figma Plugin Logo" height="24" style="max-height: 24px; max-width: 24px; margin-right: 10px;">
    View 'Klever'
  </a>
</div>

</div>
</div>
{::options parse_block_html="false" /}


## üß† Interesting Use Cases

As we applied AppAgent for Figma to several prototypes, we discovered some truly fascinating cases. These cases demonstrated that AppAgent could evolve beyond a simple lab model to become a design productivity tool capable of predicting actual user behavior and identifying potential design issues.

{::options parse_block_html="true" /}
<div class="timeline-section full-width"> 
<div class="outer">
<div class="timeline-item">
<span class="timeline-date">Use Case #1</span>

### Discovering Hidden Interactions

{::options parse_block_html="false" /}
{% include post-components/gallery.html
   columns = 2
   full_width = false
   images = "../images/projects/2024-klever/5_demo_case study_1_1.png, ../images/projects/2024-klever/5_demo_case study_1_2.png"
   caption="AI Agent found a prototype noodle that the designer missed."
%}
{::options parse_block_html="true" /}


In a travel app prototype, we asked the AI agent to check promotions and find the cheapest hotel in Bali. Contrary to our expectations, the AI agent tried to expand the article by clicking the center instead of pressing the ‚ÄòRead more‚Äô button or the search button. This is a familiar UI pattern used in many apps, but it was mistakenly not connected in this prototype. The AI agent captured this important interaction that the designer had missed.
</div>

<div class="timeline-item">
<span class="timeline-date">Use Case #2</span>
### Identifying UI Element Ambiguity


{::options parse_block_html="false" /}
{% include post-components/gallery.html
   columns = 2
   full_width = false
   images = "../images/projects/2024-klever/5_demo_case study_2_1.png, ../images/projects/2024-klever/5_demo_case study_2_2.png"
   caption="AI Agent discovered the confusing visual design of the UI components."
%}
{::options parse_block_html="true" /}

On the hotel details screen of the same prototype, we instructed the AI agent to find more information. We expected the AI to expand the article for ‚ÄòRead more‚Äô, but it repeatedly clicked the ‚ÄòDetail‚Äô tab instead. This shows that the AI agent, like a person, confused tabs and buttons due to their similar visual designs. It highlights how the AI agent can spot visual design details that the designer might have overlooked.
</div>
<div class="timeline-item">
<span class="timeline-date">Use Case #3</span>
### Predicting Hidden Features

AI Agent predicted the next UI and performed the action.

{::options parse_block_html="false" /}
{% include post-components/gallery.html
   columns = 2
   full_width = false
   images = "../images/projects/2024-klever/5_demo_case study_3_1.png, ../images/projects/2024-klever/5_demo_case study_3_2.png"
   caption="AI Agent predicted the next UI and performed the action."
%}
{::options parse_block_html="true" /}

In a task to change the number of guests to 10, the AI agent clicked the ‚Äò2 guests‚Äô area instead of pressing the ‚Äò+‚Äô button. Despite being given only a single screen, the AI predicted that clicking this area would reveal additional options. This shows that the AI can understand and predict common UI patterns, not just process the given information.
</div>
</div>
</div>
</div>


These cases demonstrate that this AI-based usability testing agent can think and act like a real user, not just follow programmed rules. It provides insights that are as valuable as those from actual users. These moments were the most thrilling and fascinating for us as creators.

## üìà Impact 

Despite being a paid plugin (due to API costs), Klever has achieved large numbers in the Figma Community beyond our expectations, demonstrating strong demand for AI-powered usability testing tools:

{::options parse_block_html="false" /}
{% include post-components/quote.html
   full_width = false
   background_color = "#000"
   font_color = "#fff"
   images = "../images/projects/2024-klever/hero2.png"
   text = " It still keeps growing every week! <br/><br/>üëÄ <span style='font-size: 24px; background-color: #333; color: #fff; padding: 2px 4px;'>2,093</span> Views<br>
üë• <span style='font-size: 24px; background-color: #333; color: #fff; padding: 2px 4px;'>1,076</span> Users<br>
üîñ <span style='font-size: 24px; background-color: #333; color: #fff; padding: 2px 4px;'>248</span> Saves"
%}
{::options parse_block_html="true" /}
<br><br>
These numbers significantly exceed our expectations for a **paid** plugin, validating our vision of making AI usability testing more accessible to designers. The growing user base and engagement metrics demonstrate that there's a real need in the design community for tools that can streamline the usability testing process while leveraging AI capabilities.

## ü§ù Community Impact

We actively shared our project's journey and findings with the design community, believing that open collaboration leads to better solutions. From local meetups to major conferences, each sharing session not only helped us gather valuable feedback but also inspired others to explore the possibilities of AI in design. This open approach led to various adaptations of our project, including an internal version at Grab. Here's our community engagement timeline:


{::options parse_block_html="true" /}
<div class="timeline-section full-width align-left">
<div class="outer">

<div class="timeline-item">
<span class="timeline-date">Jun 2024: Community Contributions #1</span>
### Friends of Figma Seoul & Educators event

At the [Friends of Figma Seoul & Educators event](https://friends.figma.com/events/details/figma-seoul-presents-product-design-education-in-ai-era-figma-meetup-in-korea/cohost-education), we presented our session "Experimenting with user test automation with AI and Open Source". It was our first time sharing this project with the design community, and the enthusiastic response encouraged us to develop it further into a Figma plugin.

{::options parse_block_html="false" /}
<iframe width="560" height="315" src="https://www.youtube.com/embed/jZgybbhoeaQ?si=iETxISV7RwDMgCDP" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
{::options parse_block_html="true" /}
</div>

<div class="timeline-item">
<span class="timeline-date">Aug 2024: Community Contributions #2</span>
### InfCon 2024 Presentation

Shortly after releasing the Figma plugin, an unexpected opportunity came our way. [Kim Ji-hong](https://www.linkedin.com/in/jihere1001/) from [Design Spectrum](https://medium.com/u/d906e16642cf) and [Hong Yeon-ui](https://www.linkedin.com/in/yeoneui-hong/) from [Inflearn](https://medium.com/u/785769823235), who had been following our project with interest, invited us to speak at [InfCon 2024](https://www.inflearn.com/conf/infcon-2024/).

{::options parse_block_html="false" /}
<iframe width="560" height="315" src="https://www.youtube.com/embed/JVfpIy5LhJ8?si=KpLw_WP7LKsMrLwL" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
<br><br>
Most importantly, we extend our heartfelt thanks to the Korean designers who showed interest in our project and asked endless questions late into Friday evening. The support and energy we received from InfCon 2024 were truly uplifting.


{% include post-components/gallery.html
   columns = 3
   full_width = false
   images = "../images/projects/2024-klever/6_infcon_1.png, ../images/projects/2024-klever/6_infcon_2.png, ../images/projects/2024-klever/6_infcon_3.png"
   caption="InfCon24 recap: We never imagined the room would be packed for our 5 PM session on a Friday‚Ä¶ <br>Thank you so much folks!"
%}
{::options parse_block_html="true" /}
</div>

<div class="timeline-item">
<span class="timeline-date">Aug 2024: Open source contributions</span>
### Challenges Ahead‚Ä¶

While there were moments of excitement and joy, there were also some realistic challenges to address. Unfortunately, we couldn‚Äôt overcome all these challenges. Crucially, because it was impossible to replace Selenium with just the API provided by Figma, the Klever plugin was limited to performing usability tests on only one screen, not the entire flow of prototype screens.

We tried requesting the teams at Figma to add some new APIs for controlling the Figma prototype screen, but unfortunately, these requests have not been properly reflected yet.

Instead of being discouraged, we decided to acknowledge these limitations and find ways to overcome them with the open-source community. So we made the source code of the Klever plugin and the AppAgent scripts, which we explained earlier all open-source.

You can check the source code through the links below:

{::options parse_block_html="false" /}
<div style="border: 1px solid #ccc; border-radius: 8px; padding: 16px; margin: 20px 0; display: flex; align-items: center; background-color: #f9f9f9;">
    <img src="https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg" alt="GitHub Icon" style="width: 40px; height: 40px; margin-right: 16px;">
    <div>
        <p style="margin: 0; font-size: 16px; font-weight: bold;">A Figma plugin that allows you to do a simple usability test in Figma</p>
        <a href="https://github.com/FigmaAI/klever" style="color: #0366d6; text-decoration: none;">github.com/FigmaAI/klever</a>
    </div>
</div>

<div style="border: 1px solid #ccc; border-radius: 8px; padding: 16px; margin: 20px 0; display: flex; align-items: center; background-color: #f9f9f9;">
    <img src="https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg" alt="GitHub Icon" style="width: 40px; height: 40px; margin-right: 16px;">
    <div>
        <p style="margin: 0; font-size: 16px; font-weight: bold;">AppAgent: Multimodal Agents as Smartphone Users</p>
        <a href="https://github.com/FigmaAI/AppAgent" style="color: #0366d6; text-decoration: none;">github.com/FigmaAI/AppAgent</a>
    </div>
</div>
</div>
{::options parse_block_html="true" /}

<div class="timeline-item">
<span class="timeline-date">Aug 2024</span>
### Community Contributions

Thankfully, a few people have suggested new approaches based on this open source, and we are conducting experiments together. One notable experiment includes integrating the Figma plugin with a Python Flask server approach:

{::options parse_block_html="false" /}
{% include post-components/gallery.html
   columns = 1
   full_width = false
   images = "https://github.com/FigmaAI/AppAgent/blob/main/assets/KleverOnServer.gif?raw=true"
   caption="Prototype of the Figma plugin via integrating with Python Flask server approach ([Code](https://github.com/FigmaAI/AppAgent))"
%}
{::options parse_block_html="true" /}
</div>
</div>

## Conclusion

This project started as a small side project for designers to learn AI while having fun. There were many challenges in the experimental process, but we received interest and encouragement from various designers and communities and motivated by all of that, we persistently repeated our experiments and eventually achieved the result of making it into a product.

In the era of AI, new AI tools are pouring out every day. Some may feel pressured to learn new things, Or some may feel a vague fear that AI will replace our design jobs. But what this project has taught us is that AI is fascinating and fun. If we learn while playing and work while learning, just like children playing with interesting toys, and improve the small inconveniences in our product design ecosystem with AI, AI will no longer be a difficult or scary entity.

We hope that these fun experiences we felt are well conveyed to all of you reading this article. So, enjoy a fun design experience with AI! ü§ó

![img](../images/projects/2024-klever/6_logo_1_4.png)

Testing UI usability with AI in just seconds. ( generated with [Microsoft Designer](https://designer.microsoft.com/) )

## Reference

- https://appagent-official.github.io/
- https://github.com/FigmaAI/klever
- https://www.figma.com/community/plugin/1383457529531594701/klever-instant-usability-testing
- https://www.inflearn.com/conf/infcon-2024/session-detail/894/
- https://www.selenium.dev/